{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1nMaBQ7g88duMECw_B4mH8ruxwnIwVzGP\" width=300/> \n",
    "\n",
    "# ML-1: Standardization, Classification & Model evaluation\n",
    "## Homework 3: Are you mad enough to sell more clothes?\n",
    "\n",
    "**ML-1 Cohort 1** <br>\n",
    "**Instructor: Dr. Rahul Dave**<br>\n",
    "**Max Score: 100** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name of people who have worked on this homework:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "* [HW-3: Are you mad enough to sell more clothes?](#HW-3:-Are-you-mad-enough-to-sell-more-clothes?)\n",
    "  * [Instructions](##Instructions)\n",
    "  * [Overview](##Overview)\n",
    "  * [Q1: Test & Training Sets & Standardization](##Q1:-Test-&-Training-Sets-&-Standardization)\n",
    "    * [1.1 Train-Test Split](###1.1-Train-Test-Split)\n",
    "    * [1.2 Standardizing data](##1.2-Standardizing-data)\n",
    "  * [Q2: Classification with Lasso (L1) regularization](##Q2:-Classification-with-Lasso-(L1)-regularization)\n",
    "    * [2.1 Baseline Classifiers](###2.1-Baseline-Classifiers)\n",
    "    * [2.2 Classification Model - Logistic with Lasso (L1)](###2.2-Classification-Model-Logistic-with-Lasso-(L1))\n",
    "    * [2.3 Prediction](###2.3-Prediction)\n",
    "  * [Q3: Confusion Matrix, Costs, Averge Profit & Thresholds](##Q3:-Confusion-Matrix,-Costs,-Averge-Profit-&-Thresholds)\n",
    "    * [3.1 Confusion Matrix](####3.1-Confusion-Matrix)\n",
    "    * [3.2 Cost and Utility Matrix](####3.2-Cost-and-Utility-Matrix)\n",
    "    * [3.3 Average Profit Per Person](####3.3-Average-Profit-Per-Person)\n",
    "    * [3.4 Thresholds](####3.4-Thresholds)\n",
    "  * [Q4: ROC Curves & Profit Curves](##Q4:-ROC-Curves-&-Profit-Curves)\n",
    "    * [4.1 ROC Curves](####4.1-ROC-Curves)\n",
    "    * [4.2 Profit Curves](####4.2-Profit-Curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This homework should be submitted in pairs.\n",
    "\n",
    "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "- Running cells out of order is a common pitfall in Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
    "\n",
    "- To work on the homework, you will first need to fork the repository into your GitHub account and clone it to work on it on your local computer. To submit your homework, push your homework into the same GitHub and upload the link on edStem.\n",
    "\n",
    "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
    "\n",
    "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
    "\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
    "```\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
    "\n",
    "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The data set is from a fairly high end clothing chain store in the North East. You should be familiar with this data from Exercise 3 where you learnt about feature engineering.\n",
    "\n",
    "You are a data analyst for this store. Your job is to write a report to the pointy-haired boss in which you show how you can increase the store's profit by targeting whom to send a catalog in the mail. Yes, you are in direct marketing. You are a quant amongst the \"mad men\". \n",
    "\n",
    "You need to explore and layout in simple terms, what the business needs to spend to increase its profit. In other words, you need a budget, and its your job to figure out how much as well.\n",
    "\n",
    "We'll guide you through the process. There is much more you can explore, of-course, but this homework will walk you through an entire real world classification and analysis process with a finite amount of work and computer runtime.\n",
    "\n",
    "You will\n",
    "\n",
    "1. learn how to standardize the data\n",
    "2. write a classifier on this data, including cross validation\n",
    "3. learn how to compare this classifier to baseline classifiers that you better beat using a profit metric rather than an accuracy metric\n",
    "4. understand and use prediction thresholds\n",
    "5. understand the use a ROC curve\n",
    "6. learn to use a profit curve to pick a model, thus directly reflecting the metric of importance\n",
    "\n",
    "You have been provided with the cleaned data after numerous feature transformations.\n",
    "\n",
    "The idea for this homework, and the attendant data set is taken from the book \"Data Mining Methods and Models\" by [Larose](http://www.dataminingconsultant.com/DMMM.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import the required libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats.stats import pearsonr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import seaborn as sns\n",
    "# sns.set_style(\"whitegrid\")\n",
    "# sns.set_context(\"poster\")\n",
    "\n",
    "sns.set_style()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Test & Training Sets & Standardization (15 points)\n",
    "\n",
    "We standardize test and training sets separately. Specifically, we wish to standardize the non-indicator columns on both the test and training sets, by subtracting out the mean of the training set from the value, and dividing by the standard deviation of the training set. This helps us put all the continuous variables on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Train-Test Split (5 points)\n",
    "\n",
    "**Split the data(`dftouse` dataframe) into train and test in 70:30 ratio. Print out the shape of your train and test features and labels. ('X_train','X_test','y_train','y_test')**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from csv file\n",
    "df = pd.read_csv('./data/Cleaned_clothing_store_data.csv')\n",
    "dftouse = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Standardizing data (5 points)\n",
    "\n",
    "Listed below are continuous features or variables that need to be standardized.\n",
    "\n",
    "`PERCENT_VARS`: 15 variables providing the percentages spent by the customer on specific classes of clothing, including sweaters, knit tops, knit dresses, blouses, jackets, career pants, casual pants, shirts, dresses, suits, outerwear, jewelry, fashion, legwear, and the collectibles line(`P*`, `PJACKETS` for example)\n",
    "\n",
    "`ZERO_IMPORTANT_VARS`: These are columns where the existence or lack thereof of a zero may be important in a classifier. We used our intuition to make these choices, believing that there is additional information encoded in having zeroes vs non-zeroes\n",
    "\n",
    "`STANDARDIZABLE`: These are continuous columns(features) that need to be brought to the same scale. They include `PERCENT_VARS` and `ZERO_IMPORTANT_VARS` and a few other columns as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to get the Standardizable features\n",
    "\n",
    "PERCENT_VARS=[ u'PSWEATERS', u'PKNIT_TOPS', u'PKNIT_DRES', u'PBLOUSES', u'PJACKETS', u'PCAR_PNTS', u'PCAS_PNTS', u'PSHIRTS', \n",
    "              u'PDRESSES', u'PSUITS', u'POUTERWEAR', u'PJEWELRY', u'PFASHION', u'PLEGWEAR', u'PCOLLSPND']\n",
    "ZERO_IMPORTANT_VARS = [u'PREVPD', u'AMSPEND', u'PSSPEND', u'CCSPEND', u'AXSPEND', u'RESPONDED', u'PERCRET']\n",
    "STANDARDIZABLE = PERCENT_VARS + ZERO_IMPORTANT_VARS + [u'FRE', u'MON',  u'AVRG', u'GMP', u'PROMOS', u'DAYS', u'FREDAYS', u'MARKDOWN', u'CLASSES', u'COUPONS', u'STYLES',  u'MAILED',  u'RESPONSERATE', u'HI', u'LTFREDAY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 a) Use `StandardScaler` from `sklearn.preprocessing` to \"fit\" the columns in `STANDRARDIZABLE` on the training set. Then use the resultant estimator to transform both the training and the test parts of each of the columns in the dataframe, replacing the old unstandardized values in the `STANDARDIZABLE` columns of `dftouse` by the new standardized ones.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 b) Should standardization be done on all the data or should it be done separately on the training & test sets? Explain with your reasoning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Classification with Lasso (L1) regularization (30 points)\n",
    "\n",
    "We will now take this data and write a classifier to predict the response, which is in the `RESP` column of `dftouse`. This response corresponds to asking the question: will a user targeted with our advertisement respond or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Baseline Classifiers (4 points)\n",
    "\n",
    "**2.1 a) What is the accuracy of a classifier that predicts that `no customer will respond to our mailing`? Why do you think the accuracy is the way it is?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 b) What is the accuracy of a classifier that predicts that `all customers will respond to our mailing`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Classification Model - Logistic with Lasso (L1) (21 points)\n",
    "\n",
    "**2.2 a) Train a Logistic Regression Model with L1 regularization. Take all columns as features apart from the response variable `RESP`. Fine-tune your model by performing 5-fold CV (use the helper function below) to find the best value of the hyper-parameter 'C' from the follwing values: *'{0.001,0.01,0.1,1,10,100}'*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for cross-validation\n",
    "\n",
    "def cv_optimize(clf, parameters, X, y, n_folds):\n",
    "\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    print(\"BEST\", gs.best_params_, gs.best_score_)\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 b) The Lasso implements internally, a form of feature selection by setting many coefficients to zero. Print the top 10 non-zero coefficient features from the classifier sorted by the absolute magnitude of the coefficients (highest to lowest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a notion of which features are important in the classification process by seeing how they correlate with the response. This can be done using the Pearson correlation coefficient between each of our features and the response.\n",
    "\n",
    "The Pearson correlation method is the most common method to use for numerical variables; it assigns a value between − 1 and 1, where 0 is no correlation, 1 is total positive correlation, and − 1 is total negative correlation. This is interpreted as follows: a correlation value of 0.7 between two variables would indicate that a significant and positive relationship exists between the two. A positive correlation signifies that if variable A goes up, then B will also go up, whereas if the value of the correlation is negative, then if A increases, B decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 c) Implement some code to obtain the Pearson correlation coefficient between each of our features and the response. Do this on the training set only! Create a dataframe indexed by the features, which has columns `abscorr` the absolute value of the correlation and `corr` the value of the correlation. Sort the dataframe by `abscorr`, highest first, and show the top 25 features with the highest absolute correlation.**\n",
    "\n",
    "**Is there much overlap with the feature selection performed by the LASSO?**\n",
    "\n",
    "*Hint* : Use `pearsonr` from `scipy.stats.stats`. This has already been imported at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 d) Compare the accuracy of this classifier with the `no customer responds baseline` model. Does this classifier seem worthwhile pursuing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 e) Is accuracy really the best metric to evaluate the models above? Why/Why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Prediction (5 points)\n",
    "\n",
    "**Print out the accuracy on the train and test set separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Confusion Matrix, Costs, Averge Profit & Thresholds (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Confusion Matrix (7 points)\n",
    "\n",
    "The confusion matrix is of the following form:\n",
    "\n",
    "![hwimages](./images/confusionmatrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 a) Plot the confusion matrix for the above models on the train & test data-sets using the helper function below(total 2 plots)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(model, features, labels, test=True):\n",
    "\n",
    "    # Predict the values from the dataset\n",
    "    y_pred = model.predict(features)\n",
    "    # Convert validation observations to one hot vectors\n",
    "    y_true = labels \n",
    "    # compute the confusion matrix\n",
    "    confusion_mtx = confusion_matrix(y_true, y_pred) \n",
    "\n",
    "    df_cm = pd.DataFrame(confusion_mtx, range(2),\n",
    "                      range(2))\n",
    "    sns.heatmap(df_cm, annot=True, annot_kws = {'size':15}, cmap = 'Blues',fmt = 'd',\n",
    "                norm=LogNorm(df_cm.values.min(),df_cm.values.max()),\n",
    "                cbar_kws={\"ticks\":[0,1,10,1e2,1e3,1e4]},vmin=0.001, vmax=10000)\n",
    "    plt.tight_layout()\n",
    "    if test:\n",
    "        plt.title('Confusion matrix on Test data')\n",
    "    else:\n",
    "        plt.title('Confusion matrix on Train data')\n",
    "    plt.ylabel('Observed label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylim(2,0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 b) Calculate and print out the Observed Negatives(ON), Observed Positives(OP), Predicted Negatives(PN) & Predicted Positives(PP) for the test data-set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to these four quantities, the confusion matrix gives us more details on proper classifications and mis-classifications from our classifier:\n",
    "\n",
    "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP). These are folks we correctly identified as responders,and thus sending them a mailing would result in a sale for us. True Positives are great. We do incur the cost of mailing them, but we like to because they will come into the store to buy.\n",
    "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP). False Positives incur us the cost of mailing them as well, but are not very costly. These are people who wouldnt have responded, but we sent them a mailing because our classifier mispredicted them as buyers. Thus, for them, we only incur the cost of preparing the mailing and mailing it to them.\n",
    "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN). These are folks we correctly identified as not-responding, and thus we dont waste any money on sending them a mailing. This is a great classification for us.\n",
    "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN). False negatives are VERY costly: these are folks who would have responded to us had we mailed them, but we didnt target them, leading to huge lost sales per person. Notice that our Logistic classifier has tons of False Negatives\n",
    "\n",
    "It is not enough to simply identify these categories from the confusion matrix. Rather, we want to sit down with our business team and identify the costs associated with each of the 4 classification situations above. Keep in mind that these costs might even change from year to year or even more suddenly: this is why it is important to have marketing and sales people on your data science teams. (See Patil, D. J. Building data science teams. \" O'Reilly Media, Inc.\", 2011.\n",
    " for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Cost  and Utility Matrix (6 points)\n",
    "\n",
    "We use the costs to write a **risk or cost matrix** in the same form as the confusion matrix above. \n",
    "\n",
    "![cost matrix](images/costmatrix.png)\n",
    "\n",
    "The negative of the cost matrix is called the **utility matrix or profit matrix** `u`. We shall use this in the next part of the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume the amortized cost of mailing it is \\$3. Lets assume additionally that the profit margin on a sale is 30% (we are a high end clothing chain).\n",
    "`True Negatives (TN)` cost us nothing but gain us nothing either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true negative cost\n",
    "tnc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the average cost of a sale, and the 30% profit assumption, we calculate `tpc`, the cost of a `True Positive (TP)`. Note: `tpc` must be negative, since we are talking about costs.\n",
    "\n",
    "The `tpc` takes into account the cost of mailing to the respondent & the cost of the coupon, and since our mailing works, we subtract out the profit. We use the average of the `AVRG` column(which is the average money spent by a customer on each visit) multiplied by the profit margin as the profit per person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positive cost\n",
    "mail = 3 \n",
    "profit_margin = 0.3\n",
    "tpc = round(mail - df.AVRG.mean()*profit_margin,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `False Negative (FN)` is a lost sale for us! We didnt mail them, and they didnt spend the money. They would have if we mailed them. So we lost a certain profit per such false negative! Thus the false-negative cost, given by `fnc`, is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnc = round(np.mean(df.AVRG)*profit_margin,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with `False Positives (FP)`. This is a person who would not have responded but you wasted $3 on. So the false positive cost, (`fpc`) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpc = mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TNC: 0.0 \n",
      " TPC: -31.17 \n",
      " FNC: 34.17 \n",
      " FPC: 3\n"
     ]
    }
   ],
   "source": [
    "print('TNC:', tnc,'\\n','TPC:', tpc,'\\n','FNC:', fnc,'\\n','FPC:', fpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print out the profit matrix array. Use the costs described above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Average Profit Per Person (16 points)\n",
    "\n",
    "The Average Profit Per Person can be found by multiplying the utility matrix (u) by the confusion matrix elementwise, and dividing by the sum of the elements in the confusion matrix, or the test set size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 a) Write a function to calculate the average profit per person by following the instructions below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_profit_pp(y, ypred, u):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    average_profit_pp\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    y: the observed response value (true value)\n",
    "    ypred: the predicted response value\n",
    "    u: Utility or the Profit matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score: average profit per person calculated\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 b) Calculate and Print the Avg Profit Per Person on our test data for the 2 baseline classifiers:** \n",
    "- Dont Send to Anyone Baseline Classifier (predicts that no customer will respond)\n",
    "- Send to Everyone Baseline Classifier (predicts that all customers will respond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 c) Taking the average profit per person as the comparison metric, which one of the above 2 classifiers is the one to beat?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 d) What is the Avg Profit Per Person for our Logistic-L1 classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Thresholds (6 points)\n",
    "\n",
    "The `sklearn` API function `predict` assumes a threshold probability of having a +ve sample to be 0.5; that is, if a sample has a greater than 0.5 chance of being a 1, assume it is so. In the predictions above, we've done exactly that - if probability is >= 0.5, then `ypred` is 1 else 0.\n",
    "\n",
    "By varying the threshold, the `ypred` and consequently the confusion matrix and the avg profit per person can change. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 a) The thresholds are NOT always kept at 0.5 while solving a classification problem. Why do you think this happens? Can you think of any other real world example where we would want to change the threshold from 0.5?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 b) Find out the average profit per person for a given thresholds of 0.05. You can use the helper function provided below to help with this problem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to return the predictions for a given threshold 't'\n",
    "\n",
    "def t_repredict(clf,t,xtest):\n",
    "    probs=clf.predict_proba(xtest)\n",
    "    p1 = probs[:,1]\n",
    "    ypred = (p1 > t)*1\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 c) Compare the above classifier with our baseline classifiers. What do you observe? Is this classifier worth pursuing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ROC Curves & Profit Curves (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 ROC Curves (3 points)\n",
    "\n",
    "ROC curves are a set of classifiers, many of them, each point corresponding to a different threshold. They are useful to compare classifiers to each other and also to baseline models.\n",
    "(In the standard scenario, where we used the  classifier accuracy, this threshold is implicitly set at 0.5, and we have only one point on the ROC curve.).\n",
    "\n",
    "The practical way to do this is to order the samples by probability of being positive. Then consider the sample with the highest score or highest probability of being positive. At first, only this sample is positive. Then, we take the sample with the next highest score, and call it positive. As we go down the list, we go down a threshold in score or probability. \n",
    "\n",
    "Now, for each such situation: only 1 positive, now 2 positive,....you can imagine a different classifier with a different confusion matrix. It will have its own false positives, three positives, etc. Its actually the same original classifier, but with a different threshold each time.\n",
    "\n",
    "As we keep going down the list, decreasing the threshold, more and more samples become positive, and at first, the true positives rise faster than the false positives. Once past a certain point, false positives increase faster than true positives. Now, if you want a balanced classifier, you look at this turn-around point. But if you want a classifier which penalizes false positives and false negatives differently, the point you want is different.\n",
    "\n",
    "To make a ROC curve you plot the True Positive Rate, \n",
    "\n",
    "$$TPR=\\frac{TP}{OP}$$\n",
    "\n",
    "against the False Positive Rate,\n",
    "\n",
    "$$FPR=\\frac{FP}{ON}$$\n",
    "\n",
    "as you go through this process of going down the list of samples. ROC curves are useful because they calculate one classifier per threshold and show you where you are in TPR/FPR space without making any assumptions about the utility matrix or which threshold is appropriate.\n",
    "\n",
    "A rote reading of the ROC curve (go to the \"northwest\" corner) is a bad idea: you must fold in the curve with any assumptions you are making about the utilities. In our case we have both an asymmetric data set, and asymmetric risk, so the north west corner may not be the right spot. Still, on the whole, a curve with a greater AUC (area under curve), or further away from the line of randomness, will give us a rough idea of what might be a better classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the ROC curves for the Logistic-L1 model. Use the helper function below given below to help you out with this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot the ROC Curve\n",
    "\n",
    "def make_roc(clf, ytest, xtest):\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(12,8))\n",
    "    fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve (area = %0.2f)' % (roc_auc))\n",
    "    \n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    for k in range(0, fpr.shape[0],200):\n",
    "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "        threshold = str(np.round(thresholds[k], 2))\n",
    "        ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "        \n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='No Model')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    \n",
    "    return ax\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Profit Curves (17 points)\n",
    "\n",
    "Just like in a ROC curve, we go down the sorted (by score or probability) list of samples. We one-by-one add an additional sample to our positive samples, noting down the classifier's TPR and FPR and threshold. In addition to what we do for the ROC curve, we now also note down the percentage of our list of samples predicted as positive. Remember we start from the most positive, where the percentage labelled as positive would be minuscule, like 0.1 or so and the threshold like a 0.99 in probability or so. As we decrease the threshold, the percentage predicted to be positive clearly increases until everything is predicted positive at a threshold of 0. What we now do is, at each such additional sample/threshold (given to us by the `roc_curve` function from `sklearn`), we calculate the expected profit per person and plot it against the percentage predicted positive by that threshold to produce a profit curve. Thus, small percentages correspond to samples most likely to be positive: a percentage of 8% means the top 8% of our samples ranked by likelihood of being positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 a) Write a function to calculate the percentage of samples classified/predicted as positive given the TPR and FPR of a classifier. Follow the instructions below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(tpr, fpr, priorp, priorn):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    percentage\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    tpr: True Positive Rate\n",
    "    fpr: False Positive Rate\n",
    "    priorp: the probability of observed +ives (OP) on our test set\n",
    "    priorn: the probability of observed +ives (ON) on our test set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    perc: percentage of samples classified as positive\n",
    "    \"\"\"\n",
    "    #your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 b) Write a function to calculate the Expected Profit Per Person given the TPR & FPR from a classifier(this is different than our `average_profit_pp` above as we now want this in terms of TPR and FPR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_profit(tpr, fpr, util, priorp, priorn):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    percentage\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    tpr: True Positive Rate\n",
    "    fpr: False Positive Rate\n",
    "    util: Utility Matrix\n",
    "    priorp: the probability of observed +ives (OP) on our test set\n",
    "    priorn: the probability of observed +ives (ON) on our test set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    profit: the average profit per person at this (fpr, tpr) point in this ROC space.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    see make_profit below for an example of how this is used\n",
    "    \"\"\"\n",
    "    #your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 c) Plot the Profit Curve for our Logistic-L1 model using the helper function below. Also plot the profit lines for the STE(Send to Everyone) and the DSTE (Dont Send to Anyone) baseline classifiers. Make sure the labels are appropriately named and the legend is clearly displayed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot the profit curve\n",
    "\n",
    "def make_profit(clf, ytest, xtest, util):\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(12,8))\n",
    "    fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    priorp=np.mean(ytest)\n",
    "    priorn=1. - priorp\n",
    "    ben=[]\n",
    "    percs=[]\n",
    "    \n",
    "    for i,t in enumerate(thresholds):\n",
    "        perc=percentage(tpr[i], fpr[i], priorp, priorn)\n",
    "        ev = av_profit(tpr[i], fpr[i], util, priorp, priorn)\n",
    "        ben.append(ev)\n",
    "        percs.append(perc*100)\n",
    "    ax.plot(percs, ben, '-', alpha=0.3, markersize=5, label='Profit Curve for model' )\n",
    "   \n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "    boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    for k in range(0, fpr.shape[0],200):\n",
    "        #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "        threshold = str(np.round(thresholds[k], 2))\n",
    "        ax.annotate(threshold, (percs[k], ben[k]), **label_kwargs)\n",
    "                \n",
    "    ax.set_xlabel('% Predicted Positive')\n",
    "    ax.set_ylabel('Avg Profit Per Person')\n",
    "    ax.set_title('Profit Curve')\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 d) Why you only start making a profit at a certain percentage?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 e) What is the region on interest on the graph that you might want to communicate with the company managers in terms of maximizing your profit over the baseline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
